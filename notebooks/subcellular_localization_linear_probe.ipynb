{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setup\n",
        "\n",
        "Install InterProt, load ESM and SAE"
      ],
      "metadata": {
        "id": "7Eaz3UmJF7S4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "EFOR7aiGFbtW"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install git+https://github.com/etowahadams/interprot.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import os\n",
        "import torch\n",
        "import warnings\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import polars as pl\n",
        "\n",
        "from huggingface_hub import hf_hub_download\n",
        "from safetensors.torch import load_file\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV, PredefinedSplit\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoTokenizer, EsmModel\n",
        "\n",
        "from interprot.sae_model import SparseAutoencoder\n",
        "from interprot.utils import get_layer_activations\n",
        "\n",
        "ESM_DIM = 1280\n",
        "SAE_DIM = 4096\n",
        "LAYER = 28\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load ESM model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"facebook/esm2_t33_650M_UR50D\")\n",
        "esm_model = EsmModel.from_pretrained(\"facebook/esm2_t33_650M_UR50D\")\n",
        "esm_model.to(device)\n",
        "esm_model.eval()\n",
        "\n",
        "# Load SAE model\n",
        "checkpoint_path = hf_hub_download(\n",
        "    repo_id=\"liambai/InterProt-ESM2-SAEs\",\n",
        "    filename=\"esm2_plm1280_l28_sae4096.safetensors\"\n",
        ")\n",
        "sae_model = SparseAutoencoder(ESM_DIM, SAE_DIM)\n",
        "sae_model.load_state_dict(load_file(checkpoint_path))\n",
        "sae_model.to(device)\n",
        "sae_model.eval()"
      ],
      "metadata": {
        "id": "GP2EorV7GEMG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "1d39bfe1-55a4-4f7b-f29f-c390387d01f0"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-14-534163372.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpolars\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m     ) from _err\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m from pandas._config import (\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0mget_option\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mset_option\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/_config/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;34m\"warn_copy_on_write\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m ]\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdates\u001b[0m  \u001b[0;31m# pyright: ignore[reportUnusedImport]  # noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m from pandas._config.config import (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/_config/config.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m from pandas._typing import (\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/_typing.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGenerator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBitGenerator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomState\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__dir__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mpublic_symbols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'testing'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         public_symbols -= {\n\u001b[1;32m    339\u001b[0m             \u001b[0;34m\"core\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"matrixlib\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/random/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;31m# add these for module-freeze analysis (like PyInstaller)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_common\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_bounded_integers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/random/_pickle.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmtrand\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomState\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_philox\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPhilox\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_pcg64\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPCG64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPCG64DXSM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_sfc64\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSFC64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mnumpy/random/mtrand.pyx\u001b[0m in \u001b[0;36minit numpy.random.mtrand\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the subcellular localization dataset"
      ],
      "metadata": {
        "id": "ScDQGD5wWyNG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download subcellular localization dataset from https://zenodo.org/records/10631963\n",
        "!gdown https://drive.google.com/uc?id=1BG91Eu80t546q-9wIDxCaSYpzGh4lPHX\n",
        "\n",
        "data_path = \"balanced.csv\"\n",
        "df = pl.read_csv(data_path)\n",
        "df = df.filter(pl.col(\"sequence\").str.len_chars() <= 1000)\n",
        "\n",
        "train_df = df.filter(pl.col(\"set\") == \"train\")\n",
        "test_df = df.filter(pl.col(\"set\") != \"train\")\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "shgBcNlAWwFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define some helpers for linear classified training, evaluation, and inspection."
      ],
      "metadata": {
        "id": "THGgSK42Wksr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_classifier(seq_acts):\n",
        "    \"\"\"Trains a linear classifier with a predefined validation split.\n",
        "\n",
        "    Uses grid search over regularization strengths to find the best classifier.\n",
        "    \"\"\"\n",
        "    X_train = []\n",
        "    y_train = []\n",
        "    test_fold = []\n",
        "\n",
        "    for row in train_df.iter_rows(named=True):\n",
        "        X_train.append(seq_acts[row['sequence']].cpu().detach().numpy())\n",
        "        y_train.append(row['target'])\n",
        "        if row['validation']:\n",
        "            test_fold.append(0)\n",
        "        else:\n",
        "            test_fold.append(-1)\n",
        "\n",
        "    X_train = np.array(X_train)\n",
        "    y_train = np.array(y_train)\n",
        "\n",
        "    classifier = LogisticRegression(multi_class='ovr')\n",
        "    ps = PredefinedSplit(test_fold=test_fold)\n",
        "    param_grid = {'C': [0.01, 0.1, 1, 10, 100]}\n",
        "    grid_search = GridSearchCV(classifier, param_grid, cv=ps, scoring='accuracy')\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    best_classifier = grid_search.best_estimator_\n",
        "    return best_classifier, grid_search.best_params_['C'], grid_search.best_score_\n",
        "\n",
        "\n",
        "def evaluate_classifier(classifier, seq_acts):\n",
        "    \"\"\"Computes the accuracy of the classifier over a test set\"\"\"\n",
        "    X_test = []\n",
        "    y_test = []\n",
        "    for row in test_df.iter_rows(named=True):\n",
        "        X_test.append(seq_acts[row['sequence']].cpu().detach().numpy())\n",
        "        y_test.append(row['target'])\n",
        "    X_test = np.array(X_test)\n",
        "    y_test = np.array(y_test)\n",
        "\n",
        "    y_pred = classifier.predict(X_test)\n",
        "\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "def write_sorted_weights(classifier, output_dir):\n",
        "    \"\"\"Writes model weights to CSV files in sorted order.\n",
        "\n",
        "    For multi-class models, writes one CSV per class.\n",
        "    Each CSV has columns \"Index\" and \"Weight\" sorted descending.\n",
        "\n",
        "    Args:\n",
        "        output_dir: Directory to write CSV files to\n",
        "    \"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    for i, class_label in enumerate(classifier.classes_):\n",
        "        class_label = class_label.replace(\"/\", \"_\").lower()\n",
        "        output_file = os.path.join(output_dir, f\"class_{class_label}_weights.csv\")\n",
        "\n",
        "        print(f\"Class: {class_label}\")\n",
        "        class_weights = classifier.coef_[i]\n",
        "        sorted_weights = sorted(enumerate(class_weights), key=lambda x: x[1], reverse=True)\n",
        "        with open(output_file, 'w', newline='') as csvfile:\n",
        "            writer = csv.writer(csvfile)\n",
        "            writer.writerow(['Index', 'Weight'])\n",
        "            for index, weight in sorted_weights:\n",
        "                writer.writerow([index, weight])\n",
        "\n",
        "        print(pd.read_csv(output_file).head())"
      ],
      "metadata": {
        "id": "_U4huDDaWjkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ESM & SAE Inference\n",
        "\n",
        "Compute and store mean-pooled embeddings for both ESM and SAE"
      ],
      "metadata": {
        "id": "KkGwi_p-IL-g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seq_esm_acts = {}\n",
        "seq_sae_acts = {}\n",
        "\n",
        "for seq_idx, row in tqdm(enumerate(df.iter_rows(named=True)), total=len(df)):\n",
        "    seq = row['sequence']\n",
        "    esm_layer_acts = get_layer_activations(\n",
        "        tokenizer=tokenizer, plm=esm_model, seqs=[seq], layer=LAYER\n",
        "    )[0][1:-1] # Trim off BoS and EoS tokens\n",
        "\n",
        "    seq_esm_acts[seq] = torch.mean(esm_layer_acts, axis=0)\n",
        "\n",
        "    sae_acts = sae_model.get_acts(esm_layer_acts)\n",
        "    seq_sae_acts[seq] = torch.mean(sae_acts, axis=0)"
      ],
      "metadata": {
        "id": "WEKYZ1bpGaM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linear classifiers\n",
        "\n",
        "- Train classifier on ESM embedding (for accuracy comparison)\n",
        "- Train classifier on SAE embedding\n",
        "- Inspect and write the weights of the SAE classifier"
      ],
      "metadata": {
        "id": "7_NvR1ZklRNe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with warnings.catch_warnings():\n",
        "    warnings.simplefilter(\"ignore\")\n",
        "\n",
        "    print(\"Training ESM classifier\")\n",
        "    esm_classifier, _, _ = train_classifier(seq_esm_acts)\n",
        "    esm_accuracy = evaluate_classifier(esm_classifier, seq_esm_acts)\n",
        "    print(f\"ESM accuracy: {esm_accuracy}\")\n",
        "\n",
        "    print(\"Training SAE classifier\")\n",
        "    sae_classifier, _, _ = train_classifier(seq_sae_acts)\n",
        "    sae_accuracy = evaluate_classifier(sae_classifier, seq_sae_acts)\n",
        "    print(f\"SAE accuracy: {sae_accuracy}\")\n",
        "    write_sorted_weights(sae_classifier, 'weights')"
      ],
      "metadata": {
        "id": "HkDwM75sXmGA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a77659cf"
      },
      "source": [
        "%%capture\n",
        "!pip install git+https://github.com/etowahadams/interprot.git\n",
        "!pip install pandas numpy --upgrade"
      ],
      "execution_count": 11,
      "outputs": []
    }
  ]
}